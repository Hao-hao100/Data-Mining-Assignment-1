{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d59062b-f63c-433b-bee4-98eac4af5adc",
   "metadata": {},
   "source": [
    "# Population-Income Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bcf332-2974-4277-bb12-037019abea15",
   "metadata": {},
   "source": [
    "## 1.Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5595577d-82af-4cf8-b669-2816d74dd3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # read .data / .test plain-text files and convert them into a table.\n",
    "import numpy as np # using NumPy arrays\n",
    "import time # time the training and prediction speeds\n",
    "from sklearn.compose import ColumnTransformer # process the categorical and numeric columns separately, then combine them\n",
    "from sklearn.preprocessing import OneHotEncoder  # the model can only compare number，this package converts text into one-hot encoded variables.\n",
    "from sklearn.pipeline import Pipeline # build a single, fixed pipeline that bundles every preprocessing step together with the estimator\n",
    "from sklearn.tree import DecisionTreeClassifier # provide a decision-tree classifier.\n",
    "from sklearn.ensemble import RandomForestClassifier # provide a random-forest classifier.\n",
    "from sklearn.model_selection import train_test_split # split the data into training and test sets.\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report) # import evaluation metrics.\n",
    "from tqdm import tqdm # progress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce7058-6b02-4b60-9263-541d87357d6f",
   "metadata": {},
   "source": [
    "## 2.Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "729d2b73-cfdb-4b68-a672-dfccb226717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_NAMES = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
    "    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "    \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"\n",
    "] # adult.data and adult.test have no header row\n",
    "train_df = pd.read_csv(\"Census Income Data Set/adult.data\",\n",
    "                       names=COL_NAMES, skipinitialspace=True) # read the dataset file, assign column names, and automatically skip any whitespace.\n",
    "test_df  = pd.read_csv(\"Census Income Data Set/adult.test\",\n",
    "                       names=COL_NAMES, skipinitialspace=True, skiprows=1) # skip the first line '|1x3 Cross validator' in thetest set.\n",
    "df = pd.concat([train_df, test_df], ignore_index=True) # It's like combining two sheets from the same Excel file into one, \n",
    "                                                       # with the index reset to make data cleaning easier later on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a49d9-55ed-4364-83a7-fbce2f64edfe",
   "metadata": {},
   "source": [
    "## 3.Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bad4acd-09fe-4358-9e9d-eb4f386b2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(\" ?\", pd.NA, inplace=True)   # In the dataset, some missing values are denoted by “?”, replace them with the official missing value “pd.NA”.\n",
    "df.dropna(inplace=True)                # drop rows with any missing values, without creating a new table.\n",
    "df[\"income\"] = df[\"income\"].str.strip().apply(\n",
    "    lambda x: 1 if x in {\">50K\", \">50K.\"} else 0) # Change \">50k\" to 1 and \"<=50k\" to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07495600-b150-4fcf-9feb-b5e2e92a61a4",
   "metadata": {},
   "source": [
    "## 4.Split into independent and dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c494ac8-8a26-4265-9d16-94bb6e90054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"income\", axis=1) # drop the entire income column to obtain the training features.\n",
    "y = df[\"income\"] # obtain the prediction target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77412a-55d4-4187-ae25-bc8857cfa2a4",
   "metadata": {},
   "source": [
    "## 5.Identify column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "429d546c-48eb-4eeb-bbc6-0d99bacdee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to split the dataset into text columns and numeric columns\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns # for subsequent one-hot encoding\n",
    "num_cols = X.select_dtypes(exclude=\"object\").columns\n",
    "# outcome：\n",
    "# cat_cols  # Index(['workclass', 'education'], dtype='object')\n",
    "# num_cols  # Index(['age', 'fnlwgt', 'education-num'], dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c184b5-9a56-46a6-87f5-770b123ba1f3",
   "metadata": {},
   "source": [
    "## 6.Transform the dataset into one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6a136eb-b708-41f9-9aa2-39047766c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(), cat_cols), # convert this column into a one-hot sparse matrix (0/1) \n",
    "                                            # and name the operation \"cat\" for easy debugging later;\n",
    "                                            # ignore: when encountering unseen categories during inference, \n",
    "                                             # the encoder won't crash; instead, it sets the entire row to 0.\n",
    "        (\"num\", \"passthrough\", num_cols) # passthrough means numeric columns remain unchanged\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645ffbd-e0d8-49c2-93ce-866b7a7190c7",
   "metadata": {},
   "source": [
    "## 7.Split the dataset into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4290a6ac-f134-4815-b193-29d65758a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# split 80 % train and 20 % test, stratify=y to keep the high-income and low-income ratio identical in both parts, \n",
    "# and fix a random seed so the train and test assignment can be reproduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db153ca-9a13-4fe9-9d79-5d2d59ebe21c",
   "metadata": {},
   "source": [
    "## 8.Define two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9a8654d-f909-449e-b0c0-7818c44799b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"DecisionTree\": DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(\n",
    "        n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803cce4-60aa-457e-b8fa-429ae408975b",
   "metadata": {},
   "source": [
    "## 9.Train + Predict + Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dff0af45-37b0-4e2f-a028-04cfa8ac3db7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DecisionTree training: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.33it/s]\n",
      "DecisionTree predicting: 100%|████████████████████████████████████████████████████| 9769/9769 [00:42<00:00, 228.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------- DecisionTree -----------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91      7431\n",
      "           1       0.78      0.58      0.67      2338\n",
      "\n",
      "    accuracy                           0.86      9769\n",
      "   macro avg       0.83      0.76      0.79      9769\n",
      "weighted avg       0.85      0.86      0.85      9769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RandomForest training: 100%|█████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.35it/s]\n",
      "RandomForest predicting: 100%|█████████████████████████████████████████████████████| 9769/9769 [05:04<00:00, 32.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------- RandomForest -----------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91      7431\n",
      "           1       0.80      0.55      0.65      2338\n",
      "\n",
      "    accuracy                           0.86      9769\n",
      "   macro avg       0.83      0.75      0.78      9769\n",
      "weighted avg       0.85      0.86      0.85      9769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for name, clf in models.items():\n",
    "    pipe = Pipeline(steps=[(\"prep\", preprocessor), (\"clf\",   clf)])\n",
    "\n",
    "    # calculate the training time\n",
    "    t0 = time.time()\n",
    "    # pipe.fit(X_train, y_train)\n",
    "    with tqdm(total=1, desc=f\"{name} training\") as pbar:\n",
    "        pipe.fit(X_train, y_train)\n",
    "        pbar.update(1)\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # calculate the predicting time\n",
    "    t0 = time.time()\n",
    "    # y_pred = pipe.predict(X_test)\n",
    "    y_pred = []\n",
    "    with tqdm(total=X_test.shape[0], desc=f\"{name} predicting\") as pbar:\n",
    "        for idx in range(X_test.shape[0]):\n",
    "            y_pred.append(pipe.predict(X_test.iloc[[idx]])[0])\n",
    "            pbar.update(1)\n",
    "    y_pred = np.array(y_pred)\n",
    "    pred_time = time.time() - t0\n",
    "\n",
    "    # make the training model output the class-probability for every test sample: \n",
    "    # a two-column array (col 0: P(<=50K), col 1: P(>50K)) and take only column 1\n",
    "    y_prob = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    results[name] = {\n",
    "        \"accuracy\":  accuracy_score(y_test, y_pred),\n",
    "        \"precision\": precision_score(y_test, y_pred),\n",
    "        \"recall\":    recall_score(y_test, y_pred),\n",
    "        \"f1\":        f1_score(y_test, y_pred),\n",
    "        \"roc_auc\":   roc_auc_score(y_test, y_prob),\n",
    "        \"train_time\": train_time,\n",
    "        \"pred_time\": pred_time\n",
    "    }\n",
    "\n",
    "    print(f\"\\n----------------------------------- {name} -----------------------------------\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TextMining",
   "language": "python",
   "name": "textmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
